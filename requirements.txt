# flash-attn (可选，需 CUDA，安装命令：pip install flash-attn --no-build-isolation)
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl#sha256=64ba4ed4a8c4c9b79d8a0959ebe3ffd6f7ea707ae222dbebe6a864237ac0e475
accelerate==1.11.0
aiohappyeyeballs==2.6.1
aiohttp==3.13.2
aiosignal==1.4.0
annotated-types==0.7.0
anyio==4.11.0
async-timeout==5.0.1
attrs==25.4.0
av==16.0.1
boto3==1.41.1
botocore==1.41.1
certifi==2025.11.12
charset-normalizer==3.4.4
click==8.3.1
datasets==4.4.1
deepspeed==0.18.2
dill==0.4.0
einops==0.8.1
et_xmlfile==2.0.0
exceptiongroup==1.3.0
filelock==3.20.0
frozenlist==1.8.0
fsspec==2025.10.0
h11==0.16.0
hf-xet==1.2.0
hjson==3.1.0
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.36.0
idna==3.11
inquirerpy==0.3.4
Jinja2==3.1.6
jmespath==1.0.1
markdown-it-py==4.0.0
MarkupSafe==3.0.3
mdurl==0.1.2
mpmath==1.3.0
msgpack==1.1.2
multidict==6.7.0
multiprocess==0.70.18
networkx==3.4.2
ninja==1.13.0
numpy==2.1.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.2.1.3
nvidia-cufile-cu12==1.13.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-cusparselt-cu12==0.6.2
nvidia-ml-py==13.580.82
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvshmem-cu12==3.3.20
nvidia-nvtx-cu12==12.4.127
openpyxl==3.1.5
packaging==25.0
pandas==2.3.3
peft==0.18.0
pfzy==0.3.4
pillow==11.3.0
platformdirs==4.5.0
prettytable==3.17.0
prompt_toolkit==3.0.52
propcache==0.4.1
protobuf==6.33.1
psutil==7.1.3
py-cpuinfo==9.0.0
pyarrow==22.0.0
pydantic==2.12.4
pydantic_core==2.41.5
pyecharts==2.0.9
Pygments==2.19.2
python-dateutil==2.9.0.post0
pytz==2025.2
PyYAML==6.0.3
qwen-vl-utils==0.0.14
regex==2025.11.3
requests==2.32.5
rich==13.9.4
s3transfer==0.15.0
safetensors==0.7.0
simplejson==3.20.2
six==1.17.0
sniffio==1.3.1
swanlab==0.7.2
sympy==1.13.1
tokenizers==0.22.1
torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tqdm==4.67.1
transformers==4.57.1
triton==3.2.0
typing-inspection==0.4.2
typing_extensions==4.15.0
tzdata==2025.2
urllib3==2.5.0
wcwidth==0.2.14
wrapt==2.0.1
xxhash==3.6.0
yarl==1.22.0
